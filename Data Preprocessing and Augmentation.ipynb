{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d13f1-a812-4120-9bcd-7f5b5efa2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CYP data filtered by the ones with IC50 value \n",
    "file =\"./MDM2_bioactivity_data_raw.csv\"\n",
    "primarydf = pd.read_csv(file)\n",
    "\n",
    "#Drop NaN in activty column\n",
    "df_clean = primarydf.dropna(subset=[\"activity\"])\n",
    "-----------------------------------------\n",
    "# Deduplication\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from optunaz.utils.preprocessing.deduplicator import *\n",
    "\n",
    "col = \"canonical_smiles\"\n",
    "# Deduplicate based on canonical_smiles, keeping median values\n",
    "df_med = KeepMedian().dedup(df_clean, \"canonical_smiles\")\n",
    "\n",
    "# Add back assay_chembl_id and remove duplicates again to ensure unique SMILES\n",
    "df_med = (\n",
    "    df_med.merge(df_clean[[\"canonical_smiles\", \"assay_chembl_id\"]], on=\"canonical_smiles\", how=\"left\")\n",
    "          .drop_duplicates(subset=\"canonical_smiles\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "----------------------------------\n",
    "# Option1：Random Splitting\n",
    "# Step 1: Prepare the data\n",
    "X = df4outer['canonical_smiles']  # SMILES column in your dataframe\n",
    "y = df4outer['pChEMBL_gt6']  # Target variable (optional)\n",
    "\n",
    "# Step 2: Perform a random split into train-test sets\n",
    "train_size_ratio = 0.8  # Define the train-test split ratio (90%-10%)\n",
    "train_df, test_df = train_test_split(\n",
    "    df4outer,\n",
    "    test_size=1-train_size_ratio,\n",
    "    random_state=20,  # Seed for reproducibility\n",
    "    stratify=y  # Stratify the split based on the target variable (optional)\n",
    ")\n",
    "\n",
    "# Step 3: Reset indices for train and test sets\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Step 4: Verify the splits\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "------------------------------------------------------------\n",
    "\n",
    "# Option2：Scaffold Splitting\n",
    "from optunaz.utils.preprocessing.splitter import ScaffoldSplit\n",
    "\n",
    "# Step 1: Configure the splitter\n",
    "scaffold_splitter = ScaffoldSplit(\n",
    "    bins='fd',  # Default binning algorithm\n",
    "    random_state=20,  # Seed for reproducibility\n",
    "    make_scaffold_generic=True,  # Make Murcko scaffolds gZeneric\n",
    "    butina_cluster=0.4,  # Clustering threshold to aggregate scaffolds\n",
    "    name='ScaffoldSplit'\n",
    ")\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "X = df4outer['canonical_smiles']  # SMILES column in your dataframe\n",
    "y = df4outer['pChEMBL_gt6']  # Target variable (optional, for stratification)\n",
    "\n",
    "# Step 3: Generate scaffold groups\n",
    "scaffold_groups = scaffold_splitter.groups(df4outer, smiles_col='canonical_smiles')\n",
    "scaffold_groups_series = pd.Series(scaffold_groups)\n",
    "\n",
    "\n",
    "# Step 4: Split the data into train-test indices using the scaffold groups\n",
    "train_indices, test_indices = scaffold_splitter.split(X, y, groups=scaffold_groups_series)\n",
    "\n",
    "# Step 5: Adjust train-test split ratio to 90%-10%\n",
    "train_size = int(len(df4outer) * 0.9)  # Calculate 90% of the data\n",
    "test_size = len(df4outer) - train_size  # Calculate remaining 10%\n",
    "\n",
    "# Select the first `train_size` samples for training and the rest for testing\n",
    "train_indices = train_indices[:train_size]\n",
    "test_indices = test_indices[:test_size]\n",
    "\n",
    "# Step 6: Create train and test datasets\n",
    "train_df = df4outer.iloc[train_indices].reset_index(drop=True)\n",
    "test_df = df4outer.iloc[test_indices].reset_index(drop=True)\n",
    "-----------------------------------------------------\n",
    "\n",
    "# Data Augmentation\n",
    "#Step 1: Gather a Large Pool of Compounds\n",
    "file_path = \"C:\\\\Users\\\\jen\\\\Downloads\\\\模型\\\\data_5cs_smiles.txt\" \n",
    "# Load the dataset with column names\n",
    "columns = [\n",
    "    \"Uniprot_Accession\", \"Molecule_ChEMBL_ID\", \"canonical_smiles\", \n",
    "    \"standard_inchi_key\", \"activity\", \"potential_duplicate\", \n",
    "    \"standard_type\", \"doc_id\", \"src_id\", \"src_description\", \"src_short_name\"\n",
    "]\n",
    "\n",
    "pool_df = pd.read_csv(file_path, delimiter=\"\\t\", names=columns, header=0)\n",
    "# Fraction of molecules to select\n",
    "fraction_to_sample = 0.2  # 20% of the molecules\n",
    "\n",
    "# Create a random sample of 20% of the molecules\n",
    "sub_library_df = pool_df1.sample(frac=fraction_to_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the sub-library to a CSV file (optional)\n",
    "sub_library_df.to_csv(\"sub_library_pool_df1.csv\", index=False)\n",
    "\n",
    "# Output the size of the new sub-library\n",
    "print(f\"Generated a sub-library with {len(sub_library_df)} molecules (20% of pool_df1).\")\n",
    "\n",
    "# calculate number of putative inactive required\n",
    "num_active = len(train_df[train_df[\"pChEMBL_gt6\"] == 1].copy())\n",
    "num_inactive = len(train_df[train_df[\"pChEMBL_gt6\"] == 0].copy())\n",
    "desired_num_inactives = num_active *100 - num_inactive # Target 100:1 ratio\n",
    "print(f\"The number of active: {num_active}, The number of inactive: {num_inactive}, Desired number of inactives: {desired_num_inactives}\")\n",
    "\n",
    "# Generating fingerprint in advance\n",
    "from tqdm import tqdm  # 导入 tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def get_fingerprint(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "tqdm.pandas(desc=\"Initializing tqdm_pandas\")\n",
    "\n",
    "active_cyp_p450 = df_med[df_med[\"pChEMBL_gt6\"] == 1].copy()\n",
    "inactive_cyp_p450 = df_med[df_med[\"pChEMBL_gt6\"] == 0].copy()\n",
    "\n",
    "print(\"Generating fingerprints for active compounds...\")\n",
    "active_cyp_p450[\"fingerprint\"] = active_cyp_p450[\"canonical_smiles\"].progress_apply(get_fingerprint)\n",
    "print(\"Generating fingerprints for inactive compounds...\")\n",
    "inactive_cyp_p450[\"fingerprint\"] = inactive_cyp_p450[\"canonical_smiles\"].progress_apply(get_fingerprint)\n",
    "\n",
    "\n",
    "# Initialize the selected inactives list\n",
    "selected_inactives = []\n",
    "\n",
    "# Iterate over pool_df1 dynamically\n",
    "for _, row in tqdm(sub_library_df.iterrows(), total=len(sub_library_df), desc=\"Processing pool\"):\n",
    "    smiles = row[\"canonical_smiles\"]\n",
    "\n",
    "    # Dynamically compute the fingerprint\n",
    "    fingerprint = get_fingerprint(smiles)\n",
    "    if fingerprint is None:\n",
    "        continue  # Skip if fingerprint cannot be generated\n",
    "\n",
    "    # Calculate max similarity to active molecules\n",
    "    max_similarity_active = max(DataStructs.BulkTanimotoSimilarity(fingerprint, active_fps))\n",
    "\n",
    "    # Skip if too similar to active compounds\n",
    "    if max_similarity_active < 0.4:\n",
    "        # Calculate max similarity to inactive molecules\n",
    "        max_similarity_inactive = max(DataStructs.BulkTanimotoSimilarity(fingerprint, inactive_fps))\n",
    "\n",
    "        # Check the threshold for inactive similarity\n",
    "        if max_similarity_inactive <= 0.99:\n",
    "            selected_inactives.append(row)\n",
    "\n",
    "    # Stop if desired number of inactives is reached\n",
    "    if len(selected_inactives) >= desired_num_inactives:\n",
    "        break\n",
    "selected_inactives_df = pd.DataFrame(selected_inactives)\n",
    "selected_inactives_df[\"pChEMBL_gt6\"] = 0\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle  # For shuffling the DataFrame\n",
    "\n",
    "# Combine the DataFrames\n",
    "final_training_set = pd.concat([train_df, selected_inactives_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the final training set (optional)\n",
    "final_training_set = shuffle(final_training_set, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Select only the specified columns\n",
    "final_training_set = final_training_set[[\"canonical_smiles\", \"activity\", \"pChEMBL_gt6\"]]\n",
    "---------------------------------------------------------------------------\n",
    "# Split the combined training set into Training Subset and Selection Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, selection_pool = train_test_split(\n",
    "    final_training_set, \n",
    "    test_size=0.9,  # 90% for selection pool\n",
    "    stratify=final_training_set['pChEMBL_gt6'],  # Stratify based on the 'activity' column\n",
    "    random_state=20  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training Subset size: {len(train_set)}, Selection Pool size: {len(selection_pool)}\")\n",
    "\n",
    "# Save the training subset to a CSV file\n",
    "train_set.to_csv(\"./Data/MDM2_train_set_20rd.csv\", index=False) \n",
    "\n",
    "# Save the selection pool to a CSV file\n",
    "selection_pool.to_csv(\"./Data/MDM2_selection_pool_20rd.csv\", index=False)\n",
    "\n",
    "test_set.to_csv(\"./Data/MDM2_test_set_20rd.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
